{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b6ee7a-6b3f-4031-9e93-fc0f5769b48e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 00 Setup\n",
    "\n",
    "- Adds parent folder of `src/` to the path. In calling code, import from `src/` with:\n",
    "    - `from src import <module>`\n",
    "    - `from src.mymodule import myfunc`\n",
    "- Starts up spark session in local environment only (not needed in Databricks)\n",
    "- Sets random seed for all operations in the driver\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37aa7728-f3ca-48a3-8d8c-6e9d749b0e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b064288-3cbd-48da-ac1e-4279a6f90a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "print(\"üé≤ Set random.seed(0) and np.random.seed(0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee69e8a3-ace5-46e6-9bc4-b5e5b3bcda74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "parent_path = str(Path(os.getcwd()).parent)\n",
    "if parent_path not in sys.path:\n",
    "    sys.path.insert(0, parent_path)\n",
    "\n",
    "print(f\"‚úÖ Added {parent_path} to path\")\n",
    "print(\"Ready to import from src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c353afc1-b47c-4d25-9e21-29c6c0f4b066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta import configure_spark_with_delta_pip\n",
    "    from src.constants import (LOCAL_DATA_PATH, \n",
    "                               LOCAL_DATA_LAKE_PATH,\n",
    "                               LOCAL_DATA_METASTORE_PATH)\n",
    "    \n",
    "    # Mirror your cloud naming locally\n",
    "    local_data_path = Path(LOCAL_DATA_PATH)\n",
    "    data_lake_dir = Path(LOCAL_DATA_LAKE_PATH)\n",
    "    metastore_dir = Path(LOCAL_DATA_METASTORE_PATH)\n",
    "\n",
    "    # Ensure directories exist\n",
    "    data_lake_dir.mkdir(parents=True, exist_ok=True)\n",
    "    metastore_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # The configs for spark.hadoop.hive and spark.hadoop.datanucleus\n",
    "    # are added to satisfy and silence benign but annoying warnings\n",
    "    \n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"spelling-bee-solver-training-LOCAL\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", str(data_lake_dir)) \\\n",
    "        .config(\"spark.hadoop.javax.jdo.option.ConnectionURL\", f\"jdbc:derby:{metastore_dir}/metastore_db;create=true\") \\\n",
    "        .config(\"spark.hadoop.hive.stats.jdbc.timeout\", \"30\") \\\n",
    "        .config(\"spark.hadoop.hive.stats.retries.wait\", \"3000\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.schema.verification\", \"false\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.schema.verification.record.version\", \"true\") \\\n",
    "        .config(\"spark.hadoop.datanucleus.autoCreateSchema\", \"true\") \\\n",
    "        .config(\"spark.hadoop.datanucleus.schema.autoCreateTables\", \"true\") \\\n",
    "        .config(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"gzip\") \\\n",
    "        .enableHiveSupport() \n",
    "\n",
    "    print(\"Initializing Spark (this will be verbose for several seconds)...\")\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    print(\"Spark initialized! Future operations will be much quieter.\")\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "815b8313-19e5-46ff-9b12-89eeca3f0826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start spark session for local environment only\n",
    "from src.envutils import is_databricks_env\n",
    "\n",
    "if not is_databricks_env():   \n",
    "    spark = create_spark_session()\n",
    "    print(f\"‚úÖ Got or created spark session for local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "246e4ceb-27a4-41be-bfc4-62eb30f0edac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config for this notebook, possibly local only\n",
    "if not is_databricks_env():\n",
    "    print(\"üéõÔ∏è updating spark config for this notebook, reducing columnarReaderBatchSize to 1024...\")\n",
    "    spark.conf.set(\"spark.sql.parquet.columnarReaderBatchSize\", \"1024\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
