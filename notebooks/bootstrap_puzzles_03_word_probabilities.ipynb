{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f106c12-6da1-4846-9c88-80d45e5b0679",
   "metadata": {},
   "source": [
    "# bootstrap_puzzles_03_word_probabilities\n",
    "\n",
    "- load silver.word_states to df\n",
    "- drop batch_id col (not relevant for this stage)\n",
    "- split df into:\n",
    "    - df_truth (df.filter(F.col(\"label\").isNotNull())\n",
    "    - df_model (df.filter(F.col(\"label\").isNull())\n",
    "- for df_truth:\n",
    "\t- rename col: label -> probability\n",
    "\t- set source col: F.lit(\"truth\")\n",
    "\t- set model_version col: F.lit(None).cast(\"int\")\n",
    "\t- drop embedding and frequency cols\n",
    "\t- keep cols: word, letter_set, last_seen_on\n",
    "- for df_model:\n",
    "\t- pass to inference() function to get probability col: df_model = inference(df_model)\n",
    "\t- set source col: F.lit(\"model\")\n",
    "\t- set model_version col: F.when(F.lit(True), 1) in order to make column nullable\n",
    "\t- AFTER this, drop embedding, frequency, and label cols\n",
    "\t- keep cols: word, letter_set, last_seen_on\n",
    "- final_df = df_truth.union(df_model)\n",
    "- add a batch_id: \"bootstrap_puzzles_1\"\n",
    "- Save as a repartitioned table? Or partition by source at least? Gets us 2 unequal partitions.\n",
    "- Without embeddings, this should not be too memory intensive to save (115k simple rows)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bceb22a-887c-47d3-9be3-00122dd681cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run \"./00_setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc3448-b4ea-4c1e-9900-aa742a81cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from src.constants import TRAINED_MODELS_PATH\n",
    "from src.fileutils import get_local_path\n",
    "from src.sparkdbutils import create_db, write_to_table_replace_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd0e45-edd8-43e9-ab80-596675af6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Parameterize\n",
    "_SOURCE_DB_NAME = \"silver\"\n",
    "_SOURCE_TABLE_NAME = \"word_states\"\n",
    "_TARGET_DB_NAME = \"gold\"\n",
    "_TARGET_TABLE_NAME = \"word_probabilities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18401554-8015-4751-b276-c6e2ab8c0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the source table\n",
    "df = spark.sql(f\"SELECT * FROM {_SOURCE_DB_NAME}.{_SOURCE_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11386b4-4c65-4fd7-8f0f-a3ea1cee4285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the source batch_id column (not relevant to current job or batch)\n",
    "df = df.drop(\"batch_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de17258-730a-42de-b83d-a3e7968f48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into two data frames:\n",
    "# df_truth = words which have been explicitly accepted or implicitly rejected\n",
    "# These words have a probability of 0.0 or 1.0, derived from their label in silver.word_states\n",
    "\n",
    "# df_model = words which exist in bronze.words but have never come up in a puzzle yet\n",
    "# These words will be assigned a probability by the model\n",
    "\n",
    "df_truth = df.filter(F.col(\"label\").isNotNull())\n",
    "df_model = df.filter(F.col(\"label\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb5599-5dbe-452a-9e70-b15ea640fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = df_truth.withColumnRenamed(\"label\", \"probability\") \\\n",
    "                   .withColumn(\"source\", F.lit(\"truth\")) \\\n",
    "                   .withColumn(\"model_version\", F.lit(None).cast(\"int\")) \\\n",
    "                   .drop(\"embedding\", \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be447a0f-adbd-4cc7-bdb6-d0f1cb93fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create broadcast for clf and svd for the workers\n",
    "model_path = get_local_path(f\"{TRAINED_MODELS_PATH}/model_v1.joblib\")\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = joblib.load(f)\n",
    "\n",
    "svd = model['svd']\n",
    "clf = model['clf']\n",
    "\n",
    "broadcast_svd = spark.sparkContext.broadcast(svd)\n",
    "broadcast_clf = spark.sparkContext.broadcast(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f8b2b-3215-43ba-8b7d-17958ba527a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_worker_imports():\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    current_dir = Path(os.getcwd())\n",
    "    project_root = current_dir.parent\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "\n",
    "@pandas_udf(returnType=FloatType())\n",
    "def predict_probability(frequencies: pd.Series, embeddings: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts frequency and embedding column values to feature vectors\n",
    "    and passes the features to the pretrained svd and model to get\n",
    "    predicted probabilities of a positive classification.\n",
    "\n",
    "    Adds a `probabilities` column with the predicted probabilities to the df\n",
    "    and returns the df.\n",
    "    \"\"\"\n",
    "\n",
    "    # UDF functions run in separate processes with their own namespaces\n",
    "    # so it is necessary to import the classes of the SVD and classifier\n",
    "    # as though we are running a new file in a brand new process.\n",
    "    setup_worker_imports()\n",
    "    \n",
    "    from src.models.HybridFrequencyBinaryClassifier import HybridFrequencyBinaryClassifier\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    # Get model from broadcast\n",
    "    svd = broadcast_svd.value\n",
    "    clf = broadcast_clf.value\n",
    "    \n",
    "    # Convert embeddings from list format to numpy array\n",
    "    embedding_matrix = np.array(embeddings.tolist())\n",
    "\n",
    "    # Apply SVD to reduce dimensions from 768 to 50\n",
    "    reduced_embeddings = svd.transform(embedding_matrix)\n",
    "\n",
    "    # Transform frequencies with log10(frequency + 1) and reshape\n",
    "    freq_array = np.log10(frequencies.to_numpy() + 1).reshape(-1, 1)\n",
    "\n",
    "    # Concatenate frequency with reduced embeddings\n",
    "    features = np.concatenate([freq_array, reduced_embeddings], axis=1)\n",
    "\n",
    "    # Get predictions (probability of positive class)\n",
    "    probabilities = clf.predict_proba(features)[:, 1]\n",
    "\n",
    "    return pd.Series(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf3e06-8835-4186-838c-3ddcc363f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to get probabilities\n",
    "df_model = df_model.withColumn(\n",
    "    \"probability\", predict_probability(F.col(\"frequency\"), F.col(\"embedding\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722482b9-2bed-497b-a57d-ecadf88387e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.withColumn(\"source\", F.lit(\"model\")) \\\n",
    "                   .withColumn(\"model_version\", F.when(F.lit(True), 1)) \\\n",
    "                   .drop(\"embedding\", \"frequency\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678c90b-9a30-4ecd-b5da-290d9d82cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine the two data frames\n",
    "final_df = df_truth.union(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f74791-d56a-4d09-89f4-252682543b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a batch_id: \"bootstrap_puzzles_1\"\n",
    "BATCH_ID = \"bootstrap_puzzles_1\"\n",
    "final_df = final_df.withColumn(\"batch_id\", F.lit(BATCH_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1fb86-300a-4d6c-9daa-9fe6359fc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create db if it doesn't exist\n",
    "create_db(spark, _TARGET_DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bdc44-e701-4fc4-b819-af786ad6d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to the table (this has the effect of creating it)\n",
    "replace_where_dict = {\"batch_id\": \"bootstrap_puzzles_1\" }\n",
    "write_to_table_replace_where(spark, \n",
    "                             final_df, \n",
    "                             _TARGET_DB_NAME, \n",
    "                             _TARGET_TABLE_NAME, \n",
    "                             replace_where_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8cb3a-84ca-4733-ab20-96974a5c9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(f\"SELECT * FROM {_TARGET_DB_NAME}.{_TARGET_TABLE_NAME}\")\n",
    "print(df2.count())\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
