{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7333cdbc-127a-4d26-a6fd-ef553ceeed56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# bootstrap_puzzles_03_word_probabilities\n",
    "\n",
    "- load silver.word_states to df\n",
    "- drop batch_id col (not relevant for this stage)\n",
    "- split df into:\n",
    "    - df_truth (df.filter(F.col(\"label\").isNotNull())\n",
    "    - df_model (df.filter(F.col(\"label\").isNull())\n",
    "- for df_truth:\n",
    "\t- rename col: label -> probability\n",
    "\t- set source col: F.lit(\"truth\")\n",
    "\t- set model_version col: F.lit(None).cast(\"int\")\n",
    "\t- drop embedding and frequency cols\n",
    "\t- keep cols: word, letter_set, last_seen_on\n",
    "- for df_model:\n",
    "\t- pass to inference() function to get probability col: df_model = inference(df_model)\n",
    "\t- set source col: F.lit(\"model\")\n",
    "\t- set model_version col: F.when(F.lit(True), 1) in order to make column nullable\n",
    "\t- AFTER this, drop embedding, frequency, and label cols\n",
    "\t- keep cols: word, letter_set, last_seen_on\n",
    "- final_df = df_truth.union(df_model)\n",
    "- add a batch_id: \"bootstrap_puzzles_1\"\n",
    "- Save as a repartitioned table? Or partition by source at least? Gets us 2 unequal partitions.\n",
    "- Without embeddings, this should not be too memory intensive to save (115k simple rows)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a8e215-25c8-4203-9fba-3d37b7f1b758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run \"./00_setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1717401f-75bc-4a24-a6e8-e7d411926b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.pandas.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from src.constants import TRAINED_MODELS_PATH\n",
    "from src.fileutils import get_local_path\n",
    "from src.sparkdbutils import create_db, write_to_table_replace_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d9d2211-7880-48a4-929e-26f01dfbd6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Parameterize\n",
    "_SOURCE_DB_NAME = \"silver\"\n",
    "_SOURCE_TABLE_NAME = \"word_states\"\n",
    "_TARGET_DB_NAME = \"gold\"\n",
    "_TARGET_TABLE_NAME = \"word_probabilities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6b83ac-0299-4188-afa7-22d9cc7d1302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the source table\n",
    "df = spark.sql(f\"SELECT * FROM {_SOURCE_DB_NAME}.{_SOURCE_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "287ea0d3-e036-46f3-af97-1d7ca0a927e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the source batch_id column (not relevant to current job or batch)\n",
    "df = df.drop(\"batch_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a51021ff-7bce-4230-8ed9-aa16e9e6854a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into two data frames:\n",
    "# df_truth = words which have been explicitly accepted or implicitly rejected\n",
    "# These words have a probability of 0.0 or 1.0, derived from their label in silver.word_states\n",
    "\n",
    "# df_model = words which exist in bronze.words but have never come up in a puzzle yet\n",
    "# These words will be assigned a probability by the model\n",
    "\n",
    "df_truth = df.filter(F.col(\"label\").isNotNull())\n",
    "df_model = df.filter(F.col(\"label\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f74ff607-1092-48f2-a876-d0a3aec97d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_truth = df_truth.withColumnRenamed(\"label\", \"probability\") \\\n",
    "                   .withColumn(\"source\", F.lit(\"truth\")) \\\n",
    "                   .withColumn(\"model_version\", F.lit(None).cast(\"int\")) \\\n",
    "                   .drop(\"embedding\", \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f1c6cfd-d664-42b2-be9a-6c4d7468a3c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create broadcast for clf and svd for the workers\n",
    "model_path = get_local_path(f\"{TRAINED_MODELS_PATH}/model_v1.joblib\")\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = joblib.load(f)\n",
    "\n",
    "svd = model['svd']\n",
    "clf = model['clf']\n",
    "\n",
    "broadcast_svd = spark.sparkContext.broadcast(svd)\n",
    "broadcast_clf = spark.sparkContext.broadcast(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f343ee-9097-4c3d-8714-db2a818ce347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def setup_worker_imports():\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    current_dir = Path(os.getcwd())\n",
    "    project_root = current_dir.parent\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "\n",
    "@pandas_udf(returnType=FloatType())\n",
    "def predict_probability(frequencies: pd.Series, embeddings: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Converts frequency and embedding column values to feature vectors\n",
    "    and passes the features to the pretrained svd and model to get\n",
    "    predicted probabilities of a positive classification.\n",
    "\n",
    "    Adds a `probabilities` column with the predicted probabilities to the df\n",
    "    and returns the df.\n",
    "    \"\"\"\n",
    "\n",
    "    # UDF functions run in separate processes with their own namespaces\n",
    "    # so it is necessary to import the classes of the SVD and classifier\n",
    "    # as though we are running a new file in a brand new process.\n",
    "    setup_worker_imports()\n",
    "    \n",
    "    from src.models.HybridFrequencyBinaryClassifier import HybridFrequencyBinaryClassifier\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    # Get model from broadcast\n",
    "    svd = broadcast_svd.value\n",
    "    clf = broadcast_clf.value\n",
    "    \n",
    "    # Convert embeddings from list format to numpy array\n",
    "    embedding_matrix = np.array(embeddings.tolist())\n",
    "\n",
    "    # Apply SVD to reduce dimensions from 768 to 50\n",
    "    reduced_embeddings = svd.transform(embedding_matrix)\n",
    "\n",
    "    # Transform frequencies with log10(frequency + 1) and reshape\n",
    "    freq_array = np.log10(frequencies.to_numpy() + 1).reshape(-1, 1)\n",
    "\n",
    "    # Concatenate frequency with reduced embeddings\n",
    "    features = np.concatenate([freq_array, reduced_embeddings], axis=1)\n",
    "\n",
    "    # Get predictions (probability of positive class)\n",
    "    probabilities = clf.predict_proba(features)[:, 1]\n",
    "\n",
    "    return pd.Series(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdea78c0-5589-4104-8876-ff83343f714d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the UDF to get probabilities\n",
    "df_model = df_model.withColumn(\n",
    "    \"probability\", predict_probability(F.col(\"frequency\"), F.col(\"embedding\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92842918-5492-41ff-9de7-bb7f318af745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_model = df_model.withColumn(\"source\", F.lit(\"model\")) \\\n",
    "                   .withColumn(\"model_version\", F.when(F.lit(True), 1)) \\\n",
    "                   .drop(\"embedding\", \"frequency\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd816ed-2ca3-4f94-8093-0b891874b44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recombine the two data frames\n",
    "final_df = df_truth.union(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c80d9f9-0605-43cb-a35a-dd3ae48488a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add a batch_id: \"bootstrap_puzzles_1\"\n",
    "BATCH_ID = \"bootstrap_puzzles_1\"\n",
    "final_df = final_df.withColumn(\"batch_id\", F.lit(BATCH_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf6e9b9d-03f1-45b8-ae1b-b611bcf119fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create db if it doesn't exist\n",
    "create_db(spark, _TARGET_DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c49dc5e5-9e71-49ed-b9b0-f7ea5a19361a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to the table (this has the effect of creating it)\n",
    "replace_where_dict = {\"batch_id\": \"bootstrap_puzzles_1\" }\n",
    "write_to_table_replace_where(spark, \n",
    "                             final_df, \n",
    "                             _TARGET_DB_NAME, \n",
    "                             _TARGET_TABLE_NAME, \n",
    "                             replace_where_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c52e89e2-e53d-44d2-b8d9-c44941ff7daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.sql(f\"SELECT * FROM {_TARGET_DB_NAME}.{_TARGET_TABLE_NAME}\")\n",
    "print(df2.count())\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bootstrap_puzzles_03_word_probabilities",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
