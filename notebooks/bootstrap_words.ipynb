{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f3fe2-4215-422a-99b7-f6bcce96b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f081c1-7df0-44ed-9386-e868e36328e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from src.wordutils import get_letter_set, filter_wordlist\n",
    "from src.fileutils import word_file_to_set, get_local_path\n",
    "from src.constants import (WORDLIST_PATH, \n",
    "                           RAW_WORDLIST_FILENAME,\n",
    "                           WORDLIST_TEMP_CSV_FILENAME,\n",
    "                           WORDS_PKL_FILENAME,\n",
    "                           WORDS_PARQUET_FILENAME,\n",
    "                           NGRAMS_API_BASE,\n",
    "                           NGRAMS_BATCH_SIZE)\n",
    "from src.ngramsutils import get_word_frequencies\n",
    "from src.embeddingutils import get_word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911562e-2ee4-4bdc-8435-2d7695bd3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the wordlist\n",
    "wordlist = filter_wordlist(word_file_to_set(f\"{WORDLIST_PATH}/{RAW_WORDLIST_FILENAME}\"))\n",
    "print(len(wordlist),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d6e8e-43c6-4998-ba5a-3da1c1b12d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp csv of word, letter_set, version for initial wordlist\n",
    "VERSION = 1\n",
    "rows = [(word, get_letter_set(word), VERSION) for word in wordlist]\n",
    "temp_path = get_local_path(f\"{WORDLIST_PATH}/{WORDLIST_TEMP_CSV_FILENAME}\")\n",
    "with open(temp_path, \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"word\", \"letter_set\", \"version\"])\n",
    "    writer.writerows(rows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c8edd-172b-4506-ad54-48e233618bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv to pandas df\n",
    "source_df = pd.read_csv(temp_path)\n",
    "source_df[\"date_added\"] = [None] * len(source_df)\n",
    "source_df[\"date_added\"] = pd.to_datetime(source_df[\"date_added\"], errors=\"coerce\")\n",
    "\n",
    "pkl_path = get_local_path(f\"{WORDLIST_PATH}/{WORDS_PKL_FILENAME}\")\n",
    "parquet_path = get_local_path(f\"{WORDLIST_PATH}/{WORDS_PARQUET_FILENAME}\")\n",
    "\n",
    "# TODO: TESTING ONLY - REMOVE\n",
    "# Add this after reading your CSV but before processing:\n",
    "print(f\"Full dataset: {len(source_df)} rows\")\n",
    "\n",
    "# Create test subset\n",
    "test_source_df = source_df.head(1000).copy()\n",
    "print(f\"Test subset: {len(test_source_df)} rows\")\n",
    "# END TODO\n",
    "\n",
    "\n",
    "def add_frequency_and_embedding(source_df: pd.DataFrame, \n",
    "                                pkl_path: str = pkl_path,\n",
    "                                resume_job: bool = False\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # Pick up job where we left off, if necessary\n",
    "    target_df = None\n",
    "    offset = 0\n",
    "    if resume_job and os.path.exists(pkl_path):\n",
    "        target_df = pd.read_pickle(pkl_path)\n",
    "        offset = len(target_df)\n",
    "        print(f\"Resuming from offset {offset} with {len(target_df)} rows already processed\")\n",
    "\n",
    "    try:\n",
    "        while offset < len(source_df):\n",
    "            print(f\"processing batch {offset}:{offset + NGRAMS_BATCH_SIZE}\")\n",
    "        \n",
    "            batch_df = source_df[offset:(offset + NGRAMS_BATCH_SIZE)].copy()\n",
    "        \n",
    "            # get words this batch\n",
    "            batch_words = list(batch_df[\"word\"].values)\n",
    "        \n",
    "            # add frequencies for this batch\n",
    "            freq_dict = get_word_frequencies(batch_words)\n",
    "            frequencies = [freq_dict[word] for word in batch_words]\n",
    "            batch_df[\"frequency\"] = frequencies\n",
    "        \n",
    "            # add embeddings for this batch\n",
    "            embeddings_dict = get_word_embeddings(batch_words)\n",
    "            embeddings = [embeddings_dict[word] for word in batch_words]\n",
    "            batch_df[\"embedding\"] = embeddings\n",
    "        \n",
    "            # Combine batches\n",
    "            if target_df is None:\n",
    "                target_df = batch_df\n",
    "            else:\n",
    "                target_df = pd.concat([target_df, batch_df], ignore_index=True)\n",
    "        \n",
    "            # Save checkpoint\n",
    "            target_df.to_pickle(\"./target_df.pkl\")\n",
    "        \n",
    "            # Quick and dirty logging, so we know where to pick up\n",
    "            print(f\"completed batch of rows {offset}-{offset + NGRAMS_BATCH_SIZE}\")\n",
    "        \n",
    "            # Finally increment the offset for the next iteration\n",
    "            offset += NGRAMS_BATCH_SIZE\n",
    "\n",
    "        print(f\"Processing complete! Final dataset has {len(target_df)} rows\")\n",
    "        return target_df\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Exception occurred! Resume job at offset {offset}\")\n",
    "        print(f\"Current progress: {len(target_df) if target_df is not None else 0} rows processed\")\n",
    "        raise(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768e896-5853-4977-a77d-aa9b8ccffa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run - comment out before rerunning\n",
    "# result_df = add_frequency_and_embedding(source_df)\n",
    "\n",
    "# Resume if it crashed - uncomment to use\n",
    "# result_df = add_frequency_and_embedding(source_df, resume_job=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8141d-b94b-4a7d-88e5-35da845ffc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9586a8b-8523-4170-8b83-d2d1cd43ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431001e0-980a-4c5f-82f6-6fc925ece8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive validation checks for your word embeddings dataset\n",
    "\n",
    "def validate_processed_data(original_df, processed_df, wordlist):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of the processed word embeddings dataset\n",
    "    \"\"\"\n",
    "    print(\"=== DATA VALIDATION REPORT ===\\n\")\n",
    "    \n",
    "    # 1. ROW COUNT VALIDATION\n",
    "    print(\"1. ROW COUNT VALIDATION\")\n",
    "    print(f\"   Original CSV rows: {len(original_df)}\")\n",
    "    print(f\"   Processed DF rows: {len(processed_df)}\")  \n",
    "    print(f\"   Filtered wordlist: {len(wordlist)}\")\n",
    "    \n",
    "    if len(original_df) == len(processed_df) == len(wordlist):\n",
    "        print(\"   ✅ Row counts match perfectly\")\n",
    "    else:\n",
    "        print(\"   ❌ Row count mismatch!\")\n",
    "    print()\n",
    "    \n",
    "    # 2. SCHEMA VALIDATION\n",
    "    print(\"2. SCHEMA VALIDATION\")\n",
    "    expected_columns = ['word', 'letter_set', 'version', 'date_added', 'frequency', 'embedding']\n",
    "    actual_columns = list(processed_df.columns)\n",
    "    print(f\"   Expected columns: {expected_columns}\")\n",
    "    print(f\"   Actual columns: {actual_columns}\")\n",
    "    \n",
    "    if set(expected_columns) == set(actual_columns):\n",
    "        print(\"   ✅ All expected columns present\")\n",
    "    else:\n",
    "        missing = set(expected_columns) - set(actual_columns)\n",
    "        extra = set(actual_columns) - set(expected_columns)\n",
    "        if missing: print(f\"   ❌ Missing columns: {missing}\")\n",
    "        if extra: print(f\"   ❌ Extra columns: {extra}\")\n",
    "    print()\n",
    "    \n",
    "    # 3. DATA TYPE VALIDATION\n",
    "    print(\"3. DATA TYPE VALIDATION\")\n",
    "    print(f\"   Data types:\\n{processed_df.dtypes}\")\n",
    "    \n",
    "    # Check specific types\n",
    "    checks = [\n",
    "        ('word', 'object'),\n",
    "        ('letter_set', 'object'), \n",
    "        ('version', 'int64'),\n",
    "        ('date_added', 'datetime64[ns]'),\n",
    "        ('frequency', ('float64')),\n",
    "    ]\n",
    "    \n",
    "    for col, expected_type in checks:\n",
    "        actual_type = str(processed_df[col].dtype)\n",
    "        if isinstance(expected_type, tuple):\n",
    "            if actual_type in expected_type:\n",
    "                print(f\"   ✅ {col}: {actual_type}\")\n",
    "            else:\n",
    "                print(f\"   ❌ {col}: expected {expected_type}, got {actual_type}\")\n",
    "        else:\n",
    "            if actual_type == expected_type:\n",
    "                print(f\"   ✅ {col}: {actual_type}\")\n",
    "            else:\n",
    "                print(f\"   ❌ {col}: expected {expected_type}, got {actual_type}\")\n",
    "    print()\n",
    "    \n",
    "    # 4. NULL/MISSING VALUE VALIDATION\n",
    "    print(\"4. NULL/MISSING VALUE VALIDATION\")\n",
    "    null_counts = processed_df.isnull().sum()\n",
    "    print(f\"   Null counts per column:\\n{null_counts}\")\n",
    "    \n",
    "    # Check expected nulls\n",
    "    if null_counts['date_added'] == len(processed_df):\n",
    "        print(\"   ✅ All date_added values are null (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ❌ Expected all date_added to be null, but {len(processed_df) - null_counts['date_added']} are not null\")\n",
    "    \n",
    "    # Check no unexpected nulls\n",
    "    critical_cols = ['word', 'letter_set', 'version', 'frequency', 'embedding']\n",
    "    for col in critical_cols:\n",
    "        if null_counts[col] == 0:\n",
    "            print(f\"   ✅ No nulls in {col}\")\n",
    "        else:\n",
    "            print(f\"   ❌ Found {null_counts[col]} nulls in {col}\")\n",
    "    print()\n",
    "    \n",
    "    # 5. WORD CONSTRAINT VALIDATION\n",
    "    print(\"5. WORD CONSTRAINT VALIDATION\")\n",
    "    \n",
    "    # Check word length (should be >= 4 for Spelling Bee)\n",
    "    short_words = processed_df[processed_df['word'].str.len() < 4]\n",
    "    if len(short_words) == 0:\n",
    "        print(\"   ✅ No words shorter than 4 characters\")\n",
    "    else:\n",
    "        print(f\"   ❌ Found {len(short_words)} words shorter than 4 characters\")\n",
    "        print(f\"       Examples: {list(short_words['word'].head())}\")\n",
    "    \n",
    "    # Check letter_set constraint (should be <= 7 distinct letters)\n",
    "    long_letter_sets = processed_df[processed_df['letter_set'].str.len() > 7]\n",
    "    if len(long_letter_sets) == 0:\n",
    "        print(\"   ✅ No words with more than 7 distinct letters\")\n",
    "    else:\n",
    "        print(f\"   ❌ Found {len(long_letter_sets)} words with more than 7 distinct letters\")\n",
    "        print(f\"       Examples: {list(long_letter_sets[['word', 'letter_set']].head().to_dict('records'))}\")\n",
    "    \n",
    "    # Verify letter_set calculation\n",
    "    sample_check = processed_df.head(10).copy()\n",
    "    sample_check['calculated_letter_set'] = sample_check['word'].apply(lambda w: ''.join(sorted(set(w.upper()))))\n",
    "    letter_set_matches = (sample_check['letter_set'] == sample_check['calculated_letter_set']).all()\n",
    "    if letter_set_matches:\n",
    "        print(\"   ✅ Letter set calculation appears correct (spot check)\")\n",
    "    else:\n",
    "        print(\"   ❌ Letter set calculation may be incorrect\")\n",
    "        print(f\"       Sample mismatches:\\n{sample_check[['word', 'letter_set', 'calculated_letter_set']]}\")\n",
    "    print()\n",
    "    \n",
    "    # 6. VERSION VALIDATION\n",
    "    print(\"6. VERSION VALIDATION\")\n",
    "    version_values = processed_df['version'].unique()\n",
    "    if len(version_values) == 1 and version_values[0] == 1:\n",
    "        print(\"   ✅ All versions are 1 (as expected)\")\n",
    "    else:\n",
    "        print(f\"   ❌ Expected all versions to be 1, found: {version_values}\")\n",
    "    print()\n",
    "    \n",
    "    # 7. FREQUENCY VALIDATION\n",
    "    print(\"7. FREQUENCY VALIDATION\")\n",
    "    freq_stats = processed_df['frequency'].describe()\n",
    "    print(f\"   Frequency statistics:\\n{freq_stats}\")\n",
    "    \n",
    "    # Check for reasonable frequency range\n",
    "    zero_freq = (processed_df['frequency'] == 0).sum()\n",
    "    negative_freq = (processed_df['frequency'] < 0).sum()\n",
    "    \n",
    "    print(f\"   Words with zero frequency: {zero_freq}\")\n",
    "    print(f\"   Words with negative frequency: {negative_freq}\")\n",
    "    \n",
    "    if negative_freq == 0:\n",
    "        print(\"   ✅ No negative frequencies\")\n",
    "    else:\n",
    "        print(\"   ❌ Found negative frequencies\")\n",
    "    print()\n",
    "    \n",
    "    # 8. EMBEDDING VALIDATION\n",
    "    print(\"8. EMBEDDING VALIDATION\")\n",
    "    \n",
    "    # Check embedding dimensions\n",
    "    if len(processed_df) > 0:\n",
    "        first_embedding = processed_df['embedding'].iloc[0]\n",
    "        if isinstance(first_embedding, (list, tuple, np.ndarray)):\n",
    "            embedding_dim = len(first_embedding)\n",
    "            print(f\"   Embedding dimension: {embedding_dim}\")\n",
    "            \n",
    "            if embedding_dim == 768:\n",
    "                print(\"   ✅ Embedding dimension is 768 (as expected)\")\n",
    "            else:\n",
    "                print(f\"   ❌ Expected embedding dimension 768, got {embedding_dim}\")\n",
    "            \n",
    "            # Check all embeddings have same dimension\n",
    "            dims = processed_df['embedding'].apply(lambda x: len(x) if isinstance(x, (list, tuple, np.ndarray)) else 0)\n",
    "            if dims.nunique() == 1:\n",
    "                print(\"   ✅ All embeddings have consistent dimensions\")\n",
    "            else:\n",
    "                print(f\"   ❌ Inconsistent embedding dimensions: {dims.value_counts()}\")\n",
    "            \n",
    "            # Check for null embeddings\n",
    "            null_embeddings = processed_df['embedding'].apply(lambda x: x is None or (isinstance(x, (list, tuple, np.ndarray)) and len(x) == 0))\n",
    "            if null_embeddings.sum() == 0:\n",
    "                print(\"   ✅ No null or empty embeddings\")\n",
    "            else:\n",
    "                print(f\"   ❌ Found {null_embeddings.sum()} null or empty embeddings\")\n",
    "        else:\n",
    "            print(f\"   ❌ Embedding data type unexpected: {type(first_embedding)}\")\n",
    "    print()\n",
    "    \n",
    "    # 9. WORD SET VALIDATION\n",
    "    print(\"9. WORD SET VALIDATION\")\n",
    "    original_words = set(wordlist)\n",
    "    processed_words = set(processed_df['word'])\n",
    "    \n",
    "    if original_words == processed_words:\n",
    "        print(\"   ✅ Processed words exactly match original wordlist\")\n",
    "    else:\n",
    "        missing_from_processed = original_words - processed_words\n",
    "        extra_in_processed = processed_words - original_words\n",
    "        \n",
    "        if missing_from_processed:\n",
    "            print(f\"   ❌ Missing from processed: {len(missing_from_processed)} words\")\n",
    "            print(f\"       Examples: {list(list(missing_from_processed)[:5])}\")\n",
    "        \n",
    "        if extra_in_processed:\n",
    "            print(f\"   ❌ Extra in processed: {len(extra_in_processed)} words\") \n",
    "            print(f\"       Examples: {list(list(extra_in_processed)[:5])}\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = processed_df['word'].duplicated().sum()\n",
    "    if duplicates == 0:\n",
    "        print(\"   ✅ No duplicate words\")\n",
    "    else:\n",
    "        print(f\"   ❌ Found {duplicates} duplicate words\")\n",
    "    print()\n",
    "    \n",
    "    # 10. PARQUET ROUNDTRIP VALIDATION\n",
    "    print(\"10. PARQUET ROUNDTRIP VALIDATION\")\n",
    "    \n",
    "    # This would be called after reading back from parquet\n",
    "    # Just check that the dtypes are preserved reasonably\n",
    "    print(\"   Check dtypes after parquet roundtrip:\")\n",
    "    for col in processed_df.columns:\n",
    "        print(f\"     {col}: {processed_df[col].dtype}\")\n",
    "    \n",
    "    print(\"\\n=== VALIDATION COMPLETE ===\")\n",
    "\n",
    "# Usage in your notebook:\n",
    "# validate_processed_data(source_df, df2, wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9079988-32c7-412d-8488-ebdfcea14372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove testing\n",
    "test_result_df = add_frequency_and_embedding(test_source_df)\n",
    "# Save test results\n",
    "test_parquet_path = get_local_path(f\"{WORDLIST_PATH}/test_words.parquet\")\n",
    "test_result_df.to_parquet(test_parquet_path)\n",
    "\n",
    "# Read back and validate\n",
    "test_df2 = pd.read_parquet(test_parquet_path)\n",
    "\n",
    "# Run validation (you'll need the original wordlist subset too)\n",
    "test_wordlist = list(test_source_df['word'])\n",
    "validate_processed_data(test_source_df, test_df2, test_wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80721d-ccbd-4deb-9fee-6ef60d4c818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some high-frequency words make sense\n",
    "print(\"Top 10 most frequent words:\")\n",
    "print(test_df2.nlargest(10, 'frequency')[['word', 'frequency']])\n",
    "\n",
    "# Check some low-frequency words\n",
    "print(\"\\nSample low-frequency words:\")\n",
    "print(test_df2.nsmallest(10, 'frequency')[['word', 'frequency']])\n",
    "\n",
    "# Verify letter_set examples\n",
    "print(\"\\nSample letter_set validation:\")\n",
    "sample = test_df2.sample(5)[['word', 'letter_set']]\n",
    "for _, row in sample.iterrows():\n",
    "    calculated = ''.join(sorted(set(row['word'].upper())))\n",
    "    print(f\"'{row['word']}' -> expected: '{calculated}', actual: '{row['letter_set']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92925212-8ad7-477e-8afe-1cd5cc25e099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
