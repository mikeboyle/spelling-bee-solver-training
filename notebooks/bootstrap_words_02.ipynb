{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e864cb6e-0458-4aad-bfbb-076dc00fc76a",
   "metadata": {},
   "source": [
    "# bootstrap_words_02\n",
    "- Read in parquet files from previous stage as pandas\n",
    "- ensure that vectors are lists of int32s\n",
    "- convert to pyspark with bronze_words schema and write to delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c2e33-14e9-40e9-b99f-6f04a1af0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc0e9e-0628-45da-9524-b22016cb0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.bronzeutils import validate_embeddings, words_schema\n",
    "from src.fileutils import get_local_path\n",
    "from src.constants import WORDLIST_PATH, WORDS_PARQUET_FILENAME\n",
    "from src.sparkdbutils import create_unpartitioned_table\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e15135-cd68-42e8-b99c-984385df8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START REPAIR SCRIPT\n",
    "# INPUT_FILENAME = \"words_old.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4d6bd-aacc-4d28-9378-a5a77027d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_parquet_path = get_local_path(f\"{WORDLIST_PATH}/{INPUT_FILENAME}\")\n",
    "# output_parquet_path = get_local_path(f\"{WORDLIST_PATH}/{WORDS_PARQUET_FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ed3f2-e46e-4097-b03f-6910c1e4ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(input_parquet_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe2332-eadb-4340-b975-e53b0edeaa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns=[\"date_added\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977e4ea-03e8-41c5-959d-e1cad5160c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet(output_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9adf6-9d34-4058-b3ed-475416f0b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END REPAIR SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664281a9-8a42-4dd8-9be6-4fd83e257439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parameterize _TARGET_DB_NAME in pipeline (or use as constants?)\n",
    "_TARGET_DB_NAME = \"bronze\"\n",
    "_TARGET_TABLE_NAME = \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c173ab-79f0-4f8f-aef3-dfdc733a31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = get_local_path(f\"{WORDLIST_PATH}/{WORDS_PARQUET_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d39031-f0be-4cda-9088-42287cb8c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.parquet(parquet_path, schema=words_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17ad55-38a3-4d1d-944e-9be68b78f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the df has no null elements in any embedding.\n",
    "# Then explicitly clean the nulls so we can then apply a schema\n",
    "# where embeddings cannot contain null elements\n",
    "\n",
    "null_elements_df = spark_df.filter(F.expr(\"exists(embedding, x -> x IS NULL)\"))\n",
    "\n",
    "# Count how many such rows exist\n",
    "count_null_elements = null_elements_df.count()\n",
    "\n",
    "if count_null_elements > 0:\n",
    "    raise Exception(\"Source data has null values in its embeddings\")\n",
    "else:\n",
    "    print(\"✅ No null values in any embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2490e7-a7c7-4be4-be84-3daca08d2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_schema(df, new_schema):\n",
    "    current_schema = {field.name: field.dataType for field in df.schema.fields}\n",
    "    exprs = []\n",
    "    for field in new_schema.fields:\n",
    "        if field.name in current_schema:\n",
    "            current_type = current_schema[field.name]\n",
    "            target_type = field.dataType\n",
    "\n",
    "            if current_type == target_type:\n",
    "                # Same type, no cast needed\n",
    "                exprs.append(F.col(field.name).alias(field.name))\n",
    "            else:\n",
    "                # Different types, need to cast\n",
    "                exprs.append(F.col(field.name).cast(target_type).alias(field.name))\n",
    "        else:\n",
    "            # Column in target_schema not in current df, add null with correct type\n",
    "            exprs.append(F.lit(None).cast(field.dataType).alias(field.name))\n",
    "    \n",
    "    return df.select(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b742f6-a0d1-4616-935a-44cc53958339",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = apply_schema(spark_df, words_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebe048-3ec0-48d7-b2e6-cb8dd492fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1010b67-d7fc-4317-82ed-4709558d8577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_unpartitioned_table(spark, final_df, _TARGET_TABLE_NAME, _TARGET_DB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d46b1-f6c5-4c19-87b9-4857791c462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✅ Successfully created boostrapped words table {_TARGET_DB_NAME}.{_TARGET_TABLE_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
