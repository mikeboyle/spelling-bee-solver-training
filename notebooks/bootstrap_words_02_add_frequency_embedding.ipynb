{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b79766-9507-4c1b-945d-697a7893553a",
   "metadata": {},
   "source": [
    "# bootstrap_words_02_add_frequency_embedding\n",
    "\n",
    "- read in CSV -> spark df\n",
    "- scrape word frequencies from API -> joblib dict\n",
    "- scrape word embeddings from BERT -> joblib dict\n",
    "- load frequency and embeddings, add to spark df\n",
    "- NO date added column, just use version\n",
    "- save as Delta table bronze.words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c707e-4f04-4218-84e6-ce3465bd9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ba60-708a-421e-8924-a69c75419996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from src.constants import WORDLIST_PATH, WORDLIST_TEMP_CSV_FILENAME, WORD_DATA_FILENAME\n",
    "from src.ngramsutils import get_word_frequencies_threaded\n",
    "from src.embeddingutils import get_word_embeddings\n",
    "###\n",
    "import numpy as np\n",
    "import os\n",
    "from src.wordutils import get_letter_set, filter_wordlist\n",
    "from src.fileutils import word_file_to_set, get_local_path\n",
    "from src.constants import (WORDS_PKL_FILENAME,\n",
    "                           WORDS_PARQUET_FILENAME,\n",
    "                           NGRAMS_API_BASE,\n",
    "                           NGRAMS_BATCH_SIZE)\n",
    "from src.ngramsutils import get_word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8f643-d4e9-4c47-a59a-951a4f3b700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_word_data_with_checkpoint(words_list: list, \n",
    "                                     output_path: str,\n",
    "                                     batch_size: int = 100,\n",
    "                                     resume_job: bool = False):\n",
    "    \"\"\"\n",
    "    Collect frequency and embedding data for a list of words with checkpointing.\n",
    "    Saves final results as a joblib file that can be reused for multiple purposes.\n",
    "    \n",
    "    Args:\n",
    "        words_list: List of words to process\n",
    "        output_path: Path to save final joblib file with all data\n",
    "        batch_size: Batch size for API calls (default 100 for API limits)\n",
    "        resume_job: Whether to resume from existing checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'freq_dict' and 'embeddings_dict'\n",
    "    \"\"\"\n",
    "    checkpoint_path = output_path.replace('.joblib', '_checkpoint.joblib')\n",
    "    \n",
    "    # Check for existing final output\n",
    "    if os.path.exists(output_path) and not resume_job:\n",
    "        print(f\"Final output already exists at {output_path}\")\n",
    "        with open(output_path, 'rb') as f:\n",
    "            return joblib.load(f)\n",
    "    \n",
    "    # Initialize or load checkpoint\n",
    "    freq_dict = {}\n",
    "    embeddings_dict = {}\n",
    "    start_batch = 0\n",
    "    \n",
    "    if resume_job and os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint_data = joblib.load(f)\n",
    "            freq_dict = checkpoint_data.get('freq_dict', {})\n",
    "            embeddings_dict = checkpoint_data.get('embeddings_dict', {})\n",
    "            start_batch = checkpoint_data.get('last_batch', 0) + 1\n",
    "        print(f\"Resuming from batch {start_batch} with {len(freq_dict)} words already processed\")\n",
    "    \n",
    "    total_batches = (len(words_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    try:\n",
    "        # Process remaining batches\n",
    "        for batch_idx in range(start_batch, total_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(words_list))\n",
    "            batch_words = words_list[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"Processing batch {batch_idx + 1}/{total_batches}: words {start_idx}-{end_idx-1}\")\n",
    "            \n",
    "            # Get frequencies for this batch\n",
    "            batch_freq_dict = get_word_frequencies_threaded(batch_words, max_workers=10)\n",
    "            freq_dict.update(batch_freq_dict)\n",
    "            \n",
    "            # Get embeddings for this batch\n",
    "            batch_embeddings_dict = get_word_embeddings(batch_words)\n",
    "            embeddings_dict.update(batch_embeddings_dict)\n",
    "            \n",
    "            # Save checkpoint after each batch\n",
    "            # Save embeddings as pure python lists\n",
    "            checkpoint_data = {\n",
    "                'freq_dict': freq_dict,\n",
    "                'embeddings_dict': {k: v.tolist() for k, v in embeddings_dict.items()},\n",
    "                'last_batch': batch_idx,\n",
    "                'total_batches': total_batches\n",
    "            }\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                joblib.dump(checkpoint_data, f)\n",
    "            \n",
    "            print(f\"✅ Completed batch {batch_idx + 1}/{total_batches}\")\n",
    "    \n",
    "    except Exception as err:\n",
    "        print(f\"Exception occurred! Resume job from batch {batch_idx}\")\n",
    "        print(f\"Current progress: {len(freq_dict)} words processed\")\n",
    "        raise err\n",
    "    \n",
    "    # Save final output\n",
    "    # Save embeddings not as numpy\n",
    "    final_data = {\n",
    "        'freq_dict': freq_dict,\n",
    "        'embeddings_dict': {k: v.tolist() for k, v in embeddings_dict.items()},\n",
    "        'processed_words': list(freq_dict.keys()),\n",
    "        'total_words': len(freq_dict),\n",
    "        'metadata': {\n",
    "            'batch_size': batch_size,\n",
    "            'total_batches': total_batches\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        joblib.dump(final_data, f)\n",
    "    \n",
    "    print(f\"✅ Processing complete! Saved {len(freq_dict)} words to {output_path}\")\n",
    "    \n",
    "    # Clean up checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"✅ Cleaned up checkpoint file\")\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79280c9-fbd7-4387-ba8f-5bc9bd236a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_data_to_spark_df(df, word_data_path: str):\n",
    "    \"\"\"\n",
    "    Apply frequency and embedding data to a Spark DataFrame using broadcast variables.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame with a 'word' column\n",
    "        word_data_path: Path to joblib file created by collect_word_data_with_checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame with 'frequency' and 'embedding' columns added\n",
    "    \"\"\"\n",
    "    # Load the word data\n",
    "    with open(word_data_path, 'rb') as f:\n",
    "        word_data = joblib.load(f)\n",
    "    \n",
    "    freq_dict = word_data['freq_dict']\n",
    "    embeddings_dict = word_data['embeddings_dict']\n",
    "    \n",
    "    print(f\"Loaded data for {len(freq_dict)} words from {word_data_path}\")\n",
    "    \n",
    "    # Create broadcast variables for efficient lookups\n",
    "    freq_broadcast = spark.sparkContext.broadcast(freq_dict)\n",
    "    embeddings_broadcast = spark.sparkContext.broadcast(embeddings_dict)\n",
    "    \n",
    "    frequency_udf = F.udf(lambda word: freq_broadcast.value.get(word, 0.0), FloatType())\n",
    "    embedding_udf = F.udf(lambda word: embeddings_broadcast.value.get(word, [0.0] * 768), ArrayType(FloatType()))\n",
    "    \n",
    "    # Add the new columns\n",
    "    result_df = df.withColumn(\"frequency\", frequency_udf(F.col(\"word\"))) \\\n",
    "                 .withColumn(\"embedding\", embedding_udf(F.col(\"word\")))\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bdee7-cb84-4fdd-a4b1-85cfe47c67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV directly into Spark and collect word data\n",
    "initial_schema = StructType([\n",
    "    StructField(\"word\", StringType(), True),\n",
    "    StructField(\"letter_set\", StringType(), True),\n",
    "    StructField(\"version\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "temp_path = get_local_path(f\"{WORDLIST_PATH}/{WORDLIST_TEMP_CSV_FILENAME}\")\n",
    "spark_df = spark.read.csv(temp_path, header=True, schema=initial_schema)\n",
    "\n",
    "print(f\"Read in {spark_df.count()} words from csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a42bc-aac8-41e2-b0bd-41b3179449c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [row.word for row in spark_df.select(\"word\").collect()]\n",
    "word_data_path = get_local_path(f\"{WORDLIST_PATH}/{WORD_DATA_FILENAME}\")\n",
    "\n",
    "# First run - set resume_job=False\n",
    "# word_data = collect_word_data_with_checkpoint(\n",
    "#     words_list, \n",
    "#     word_data_path,\n",
    "#     batch_size=100,\n",
    "#     resume_job=False\n",
    "# )\n",
    "\n",
    "# To resume if it crashes - set resume_job=True\n",
    "# word_data = collect_word_data_with_checkpoint(\n",
    "#     words_list, \n",
    "#     word_data_path,\n",
    "#     batch_size=100,\n",
    "#     resume_job=True\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d4e19-0282-448f-9cb6-81a214a68fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Testing purposes only -- remove later\n",
    "test_words_list = words_list[617:115111:297]\n",
    "print(f\"testing on {len(test_words_list)} words...\")\n",
    "word_data = collect_word_data_with_checkpoint(\n",
    "    test_words_list, \n",
    "    word_data_path,\n",
    "    batch_size=100,\n",
    "    resume_job=False\n",
    ")\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74093e8f-bb74-42b8-9c9a-8945d0a82cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove testing code\n",
    "with open(word_data_path, \"rb\") as f:\n",
    "    my_word_data = joblib.load(f)\n",
    "\n",
    "print(len(my_word_data['freq_dict']))\n",
    "print(len(my_word_data['embeddings_dict']))\n",
    "print(set([len(emb) for _, emb in my_word_data['embeddings_dict'].items()]))\n",
    "print(my_word_data['total_words'])\n",
    "print(len(my_word_data['processed_words']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29115f7-13bf-45d9-a409-55f47e6a1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply word data to DataFrame\n",
    "# enriched_df = apply_word_data_to_spark_df(spark_df, word_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fe252-9ccf-4fbd-af7a-3cc0ea484ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE TEST CODE\n",
    "test_df = spark_df.filter(F.col(\"word\").isin(test_words_list))\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa46b3-05bf-49b4-bf24-8dc0d6cf1715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE TEST CODE\n",
    "test_enriched_df = apply_word_data_to_spark_df(test_df, word_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f64ec-64f4-4221-b36c-f35e85ed5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE TEST CODE\n",
    "print(test_enriched_df.count())\n",
    "test_enriched_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd602595-cb6f-4d69-a5f9-7bd2f92f1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE TEST CODE\n",
    "print(test_enriched_df.filter(F.col(\"embedding\").isNotNull()).count())\n",
    "print(test_enriched_df.filter(F.col(\"frequency\").isNotNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26fd5a-80df-49dc-a9ef-addcb078c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enriched_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712fff1-4660-4fde-ad17-372b5db16561",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enriched_df.select(\"embedding\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
