{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b79766-9507-4c1b-945d-697a7893553a",
   "metadata": {},
   "source": [
    "# bootstrap_words_02_add_frequency_embedding\n",
    "\n",
    "- read in CSV -> spark df\n",
    "- scrape word frequencies from API -> joblib dict\n",
    "- scrape word embeddings from BERT -> joblib dict\n",
    "- load frequency and embeddings, add to spark df\n",
    "- NO date added column, just use version\n",
    "- save as Delta table bronze.words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c707e-4f04-4218-84e6-ce3465bd9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9d042-6708-4594-b48b-32b174dd7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Parameterize as pipeline or notebook vars\n",
    "_TARGET_DB_NAME = \"bronze\"\n",
    "_TARGET_TABLE_NAME = \"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ba60-708a-421e-8924-a69c75419996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from src.bronzeutils import words_schema\n",
    "from src.constants import WORD_DATA_FILENAME, WORDLIST_PATH, WORDLIST_TEMP_CSV_FILENAME\n",
    "from src.embeddingutils import get_word_embeddings, get_word_embeddings_threaded\n",
    "from src.fileutils import get_local_path\n",
    "from src.ngramsutils import get_word_frequencies_threaded\n",
    "from src.sparkdbutils import create_db, create_unpartitioned_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c18c2-e8bc-4c32-bfed-f2365e968553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNCTION DEFINITIONS ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8f643-d4e9-4c47-a59a-951a4f3b700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_word_data_with_checkpoint(words_list: list, \n",
    "                                     output_path: str,\n",
    "                                     batch_size: int = 100,\n",
    "                                     resume_job: bool = False):\n",
    "    \"\"\"\n",
    "    Collect frequency and embedding data for a list of words with checkpointing.\n",
    "    Saves final results as a joblib file that can be reused for multiple purposes.\n",
    "    \n",
    "    Args:\n",
    "        words_list: List of words to process\n",
    "        output_path: Path to save final joblib file with all data\n",
    "        batch_size: Batch size for API calls (default 100 for API limits)\n",
    "        resume_job: Whether to resume from existing checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'freq_dict' and 'embeddings_dict'\n",
    "    \"\"\"\n",
    "    checkpoint_path = output_path.replace('.joblib', '_checkpoint.joblib')\n",
    "    \n",
    "    # Check for existing final output\n",
    "    if os.path.exists(output_path) and not resume_job:\n",
    "        print(f\"Final output already exists at {output_path}\")\n",
    "        with open(output_path, 'rb') as f:\n",
    "            return joblib.load(f)\n",
    "    \n",
    "    # Initialize or load checkpoint\n",
    "    freq_dict = {}\n",
    "    embeddings_dict = {}\n",
    "    start_batch = 0\n",
    "    \n",
    "    if resume_job and os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint_data = joblib.load(f)\n",
    "            freq_dict = checkpoint_data.get('freq_dict', {})\n",
    "            embeddings_dict = checkpoint_data.get('embeddings_dict', {})\n",
    "            start_batch = checkpoint_data.get('last_batch', 0) + 1\n",
    "        print(f\"Resuming from batch {start_batch} with {len(freq_dict)} words already processed\")\n",
    "    \n",
    "    total_batches = (len(words_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    try:\n",
    "        # Process remaining batches\n",
    "        for batch_idx in range(start_batch, total_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(words_list))\n",
    "            batch_words = words_list[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"Processing batch {batch_idx + 1}/{total_batches}: words {start_idx}-{end_idx-1}\")\n",
    "            \n",
    "            # Get frequencies for this batch\n",
    "            batch_freq_dict = get_word_frequencies_threaded(batch_words, max_workers=10)\n",
    "            freq_dict.update(batch_freq_dict)\n",
    "            \n",
    "            # Get embeddings for this batch\n",
    "            batch_embeddings_dict = get_word_embeddings_threaded(batch_words)\n",
    "            embeddings_dict.update(batch_embeddings_dict)\n",
    "            \n",
    "            # Save checkpoint after each batch\n",
    "            # Save embeddings as pure python lists\n",
    "            checkpoint_data = {\n",
    "                'freq_dict': freq_dict,\n",
    "                'embeddings_dict': {k: v.tolist() for k, v in embeddings_dict.items()},\n",
    "                'last_batch': batch_idx,\n",
    "                'total_batches': total_batches\n",
    "            }\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                joblib.dump(checkpoint_data, f)\n",
    "            \n",
    "            print(f\"✅ Completed batch {batch_idx + 1}/{total_batches}\\n\")\n",
    "    \n",
    "    except Exception as err:\n",
    "        print(f\"Exception occurred! Resume job from batch {batch_idx}\")\n",
    "        print(f\"Current progress: {len(freq_dict)} words processed\")\n",
    "        raise err\n",
    "    \n",
    "    # Save final output\n",
    "    # Save embeddings not as numpy\n",
    "    final_data = {\n",
    "        'freq_dict': freq_dict,\n",
    "        'embeddings_dict': {k: v.tolist() for k, v in embeddings_dict.items()},\n",
    "        'processed_words': list(freq_dict.keys()),\n",
    "        'total_words': len(freq_dict),\n",
    "        'metadata': {\n",
    "            'batch_size': batch_size,\n",
    "            'total_batches': total_batches\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        joblib.dump(final_data, f)\n",
    "    \n",
    "    print(f\"✅ Processing complete! Saved {len(freq_dict)} words to {output_path}\")\n",
    "    \n",
    "    # Clean up checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"✅ Cleaned up checkpoint file\")\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79280c9-fbd7-4387-ba8f-5bc9bd236a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word_data_to_spark_df(df, word_data_path: str):\n",
    "    \"\"\"\n",
    "    Apply frequency and embedding data to a Spark DataFrame using broadcast variables.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame with a 'word' column\n",
    "        word_data_path: Path to joblib file created by collect_word_data_with_checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame with 'frequency' and 'embedding' columns added\n",
    "    \"\"\"\n",
    "    # Load the word data\n",
    "    with open(word_data_path, 'rb') as f:\n",
    "        word_data = joblib.load(f)\n",
    "    \n",
    "    freq_dict = word_data['freq_dict']\n",
    "    embeddings_dict = word_data['embeddings_dict']\n",
    "    \n",
    "    print(f\"Loaded data for {len(freq_dict)} words from {word_data_path}\")\n",
    "    \n",
    "    # Create broadcast variables for efficient lookups\n",
    "    freq_broadcast = spark.sparkContext.broadcast(freq_dict)\n",
    "    embeddings_broadcast = spark.sparkContext.broadcast(embeddings_dict)\n",
    "    \n",
    "    frequency_udf = F.udf(lambda word: freq_broadcast.value.get(word, None), FloatType())\n",
    "    embedding_udf = F.udf(lambda word: embeddings_broadcast.value.get(word, [None] * 768), ArrayType(FloatType()))\n",
    "    \n",
    "    # Add the new columns\n",
    "    result_df = df.withColumn(\"frequency\", frequency_udf(F.col(\"word\"))) \\\n",
    "                 .withColumn(\"embedding\", embedding_udf(F.col(\"word\")))\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5fc29-4870-40ba-9ce5-703c06c74049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_enriched_df(enriched_df):\n",
    "    null_elements_df = enriched_df.filter(F.expr(\"exists(embedding, x -> x IS NULL)\"))\n",
    "    count_null_elements = null_elements_df.count()\n",
    "\n",
    "    if count_null_elements > 0:\n",
    "        raise Exception(\"Source data has null values in its embeddings\")\n",
    "    else:\n",
    "        print(\"✅ No null values in any embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89370b7-036d-4a4b-b0b1-099645c3cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_schema(df, new_schema):\n",
    "    current_schema = {field.name: field.dataType for field in df.schema.fields}\n",
    "    exprs = []\n",
    "    for field in new_schema.fields:\n",
    "        if field.name in current_schema:\n",
    "            current_type = current_schema[field.name]\n",
    "            target_type = field.dataType\n",
    "\n",
    "            if current_type == target_type:\n",
    "                exprs.append(F.col(field.name).alias(field.name))\n",
    "            else:\n",
    "                exprs.append(F.col(field.name).cast(target_type).alias(field.name))\n",
    "        else:\n",
    "            exprs.append(F.lit(None).cast(field.dataType).alias(field.name))\n",
    "    \n",
    "    return df.select(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae051ac-ba08-4dfd-be8c-85f1ab93027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXECUTION ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bdee7-cb84-4fdd-a4b1-85cfe47c67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV directly into Spark and collect word data\n",
    "initial_schema = StructType([\n",
    "    StructField(\"word\", StringType(), True),\n",
    "    StructField(\"letter_set\", StringType(), True),\n",
    "    StructField(\"version\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "temp_path = get_local_path(f\"{WORDLIST_PATH}/{WORDLIST_TEMP_CSV_FILENAME}\")\n",
    "spark_df = spark.read.csv(temp_path, header=True, schema=initial_schema)\n",
    "\n",
    "print(f\"Read in {spark_df.count()} words from csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a42bc-aac8-41e2-b0bd-41b3179449c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of words from the df\n",
    "words_list = [row.word for row in spark_df.select(\"word\").collect()]\n",
    "word_data_path = get_local_path(f\"{WORDLIST_PATH}/{WORD_DATA_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac7528-5c52-419e-858a-de41ef712796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for testing only\n",
    "words_list = words_list[617:113583:385]\n",
    "spark_df = spark_df.filter(F.col(\"word\").isin(words_list))\n",
    "print(len(words_list), spark_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b7245-f53c-4fe9-bd03-42ecd1f99302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect word data (frequency from ngrams API, embeddings from local pre-trained BERT model)\n",
    "# First run - set resume_job=False\n",
    "word_data = collect_word_data_with_checkpoint(\n",
    "    words_list, \n",
    "    word_data_path,\n",
    "    batch_size=100,\n",
    "    resume_job=False\n",
    ")\n",
    "\n",
    "# To resume if it crashes - set resume_job=True\n",
    "# word_data = collect_word_data_with_checkpoint(\n",
    "#     words_list, \n",
    "#     word_data_path,\n",
    "#     batch_size=100,\n",
    "#     resume_job=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29115f7-13bf-45d9-a409-55f47e6a1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply word data to DataFrame\n",
    "enriched_df = apply_word_data_to_spark_df(spark_df, word_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1ec70-aaa6-4355-b05d-89efa83745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_enriched_df(enriched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772067c1-4258-46ab-ae21-eabdc7d75110",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_df.filter(F.col(\"frequency\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bd496a-a4a6-4332-8a58-65bb44b5cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = apply_schema(enriched_df, words_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19c18f-d65e-43f2-84ad-610cb0df455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table\n",
    "if not spark.catalog.databaseExists(_TARGET_DB_NAME):\n",
    "    create_db(spark, _TARGET_DB_NAME)\n",
    "\n",
    "create_unpartitioned_table(spark, final_df, _TARGET_TABLE_NAME, _TARGET_DB_NAME)\n",
    "\n",
    "print(f\"✅ Successfully created bootstrapped words table {_TARGET_DB_NAME}.{_TARGET_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5838e8-ee27-47b6-b237-1bdd4ccb8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(f\"SELECT * FROM {_TARGET_DB_NAME}.{_TARGET_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a02040-20ed-4e22-bc65-60104f700f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.filter(F.col(\"word\").isin(words_list)).show(386)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033342d-e3a1-4d66-a4f6-a42d085a09d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
