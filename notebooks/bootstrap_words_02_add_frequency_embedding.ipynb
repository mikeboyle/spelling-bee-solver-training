{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b79766-9507-4c1b-945d-697a7893553a",
   "metadata": {},
   "source": [
    "# bootstrap_words_02_add_frequency_embedding\n",
    "\n",
    "- read in CSV -> spark df\n",
    "- scrape word frequencies from API -> joblib dict\n",
    "- scrape word embeddings from BERT -> joblib dict\n",
    "- load frequency and embeddings, add to spark df\n",
    "- NO date added column, just use version\n",
    "- save as Delta table bronze.words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c707e-4f04-4218-84e6-ce3465bd9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ba60-708a-421e-8924-a69c75419996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from src.constants import WORDLIST_PATH, WORDLIST_TEMP_CSV_FILENAME, WORD_DATA_FILENAME\n",
    "from src.ngramsutils import get_word_frequencies_threaded\n",
    "from src.embeddingutils import get_word_embeddings\n",
    "###\n",
    "import numpy as np\n",
    "import os\n",
    "from src.wordutils import get_letter_set, filter_wordlist\n",
    "from src.fileutils import word_file_to_set, get_local_path\n",
    "from src.constants import (WORDS_PKL_FILENAME,\n",
    "                           WORDS_PARQUET_FILENAME,\n",
    "                           NGRAMS_API_BASE,\n",
    "                           NGRAMS_BATCH_SIZE)\n",
    "from src.ngramsutils import get_word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8f643-d4e9-4c47-a59a-951a4f3b700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_word_data_with_checkpoint(words_list: list, \n",
    "                                     output_path: str,\n",
    "                                     batch_size: int = 100,\n",
    "                                     resume_job: bool = False):\n",
    "    \"\"\"\n",
    "    Collect frequency and embedding data for a list of words with checkpointing.\n",
    "    Saves final results as a pickle file that can be reused for multiple purposes.\n",
    "    \n",
    "    Args:\n",
    "        words_list: List of words to process\n",
    "        output_path: Path to save final pickle file with all data\n",
    "        batch_size: Batch size for API calls (default 100 for API limits)\n",
    "        resume_job: Whether to resume from existing checkpoint\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'freq_dict' and 'embeddings_dict'\n",
    "    \"\"\"\n",
    "    checkpoint_path = output_path.replace('.joblib', '_checkpoint.joblib')\n",
    "    \n",
    "    # Check for existing final output\n",
    "    if os.path.exists(output_path) and not resume_job:\n",
    "        print(f\"Final output already exists at {output_path}\")\n",
    "        with open(output_path, 'rb') as f:\n",
    "            return joblib.load(f)\n",
    "    \n",
    "    # Initialize or load checkpoint\n",
    "    freq_dict = {}\n",
    "    embeddings_dict = {}\n",
    "    start_batch = 0\n",
    "    \n",
    "    if resume_job and os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint_data = joblib.load(f)\n",
    "            freq_dict = checkpoint_data.get('freq_dict', {})\n",
    "            embeddings_dict = checkpoint_data.get('embeddings_dict', {})\n",
    "            start_batch = checkpoint_data.get('last_batch', 0) + 1\n",
    "        print(f\"Resuming from batch {start_batch} with {len(freq_dict)} words already processed\")\n",
    "    \n",
    "    total_batches = (len(words_list) + batch_size - 1) // batch_size\n",
    "    \n",
    "    try:\n",
    "        # Process remaining batches\n",
    "        for batch_idx in range(start_batch, total_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(words_list))\n",
    "            batch_words = words_list[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"Processing batch {batch_idx + 1}/{total_batches}: words {start_idx}-{end_idx-1}\")\n",
    "            \n",
    "            # Get frequencies for this batch\n",
    "            batch_freq_dict = get_word_frequencies_threaded(batch_words, max_workers=10)\n",
    "            freq_dict.update(batch_freq_dict)\n",
    "            \n",
    "            # Get embeddings for this batch\n",
    "            batch_embeddings_dict = get_word_embeddings(batch_words)\n",
    "            embeddings_dict.update(batch_embeddings_dict)\n",
    "            \n",
    "            # Save checkpoint after each batch\n",
    "            checkpoint_data = {\n",
    "                'freq_dict': freq_dict,\n",
    "                'embeddings_dict': embeddings_dict,\n",
    "                'last_batch': batch_idx,\n",
    "                'total_batches': total_batches\n",
    "            }\n",
    "            with open(checkpoint_path, 'wb') as f:\n",
    "                joblib.dump(checkpoint_data, f)\n",
    "            \n",
    "            print(f\"✅ Completed batch {batch_idx + 1}/{total_batches}\")\n",
    "    \n",
    "    except Exception as err:\n",
    "        print(f\"Exception occurred! Resume job from batch {batch_idx}\")\n",
    "        print(f\"Current progress: {len(freq_dict)} words processed\")\n",
    "        raise err\n",
    "    \n",
    "    # Save final output\n",
    "    final_data = {\n",
    "        'freq_dict': freq_dict,\n",
    "        'embeddings_dict': embeddings_dict,\n",
    "        'processed_words': list(freq_dict.keys()),\n",
    "        'total_words': len(freq_dict),\n",
    "        'metadata': {\n",
    "            'batch_size': batch_size,\n",
    "            'total_batches': total_batches\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        joblib.dump(final_data, f)\n",
    "    \n",
    "    print(f\"✅ Processing complete! Saved {len(freq_dict)} words to {output_path}\")\n",
    "    \n",
    "    # Clean up checkpoint\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "        print(\"✅ Cleaned up checkpoint file\")\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bdee7-cb84-4fdd-a4b1-85cfe47c67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV directly into Spark and collect word data\n",
    "initial_schema = StructType([\n",
    "    StructField(\"word\", StringType(), True),\n",
    "    StructField(\"letter_set\", StringType(), True),\n",
    "    StructField(\"version\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "temp_path = get_local_path(f\"{WORDLIST_PATH}/{WORDLIST_TEMP_CSV_FILENAME}\")\n",
    "spark_df = spark.read.csv(temp_path, header=True, schema=initial_schema)\n",
    "words_list = [row.word for row in spark_df.select(\"word\").collect()]\n",
    "word_data_path = get_local_path(f\"{WORDLIST_PATH}/{WORD_DATA_FILENAME}\")\n",
    "\n",
    "print(f\"Read in {len(words_list)} words from csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d4e19-0282-448f-9cb6-81a214a68fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Testing purposes only -- remove later\n",
    "test_words_list = words_list[245:111111:350]\n",
    "print(f\"testing on {len(test_words_list)} words...\")\n",
    "word_data = collect_word_data_with_checkpoint(\n",
    "    test_words_list, \n",
    "    word_data_path,\n",
    "    batch_size=100,\n",
    "    resume_job=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### END TODO\n",
    "\n",
    "# First run - set resume_job=False\n",
    "# word_data = collect_word_data_with_checkpoint(\n",
    "#     words_list, \n",
    "#     word_data_path,\n",
    "#     batch_size=100,\n",
    "#     resume_job=False\n",
    "# )\n",
    "\n",
    "# To resume if it crashes - set resume_job=True\n",
    "# word_data = collect_word_data_with_checkpoint(\n",
    "#     words_list, \n",
    "#     word_data_path,\n",
    "#     batch_size=100,\n",
    "#     resume_job=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29115f7-13bf-45d9-a409-55f47e6a1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"✅ Saved embedding and frequency data to {word_data_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
