{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c41cbb-1b2a-46e7-bdd0-ad08a863122e",
   "metadata": {},
   "source": [
    "# train_v1\n",
    "\n",
    "Uses the labeled subset `silver.word_states` data for training purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6491a8-2869-4a73-8309-1c8494a8c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run './00_setup.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c74716-5a93-4106-8898-437f3e7ded45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainutils import convert_df_to_features_v1, evaluate_thresholds\n",
    "from src.envutils import is_databricks_env\n",
    "from src.models.HybridFrequencyBinaryClassifier import HybridFrequencyBinaryClassifier\n",
    "from pyspark.sql.types import ArrayType, DoubleType, FloatType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b29516-5690-42cc-9c9e-1090af1b4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f7a5a-f75f-4b97-afec-36880db28392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "C = 0.10\n",
    "N_COMPONENTS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9e269-b9a2-433b-a973-f7c266d69195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM silver.word_states WHERE label is not NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab497f28-00b4-4f49-9ab9-c722ea876951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfeedd1-3fe6-45f0-ab52-280c931bf381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert frequencies to log frequencies\n",
    "df = df.withColumn(\"log_frequency\", F.log10(F.col(\"frequency\") + 1))\n",
    "\n",
    "# Concatenate log_frequency + features into single feature vector\n",
    "final_df = convert_df_to_features_v1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c08946-18db-417a-93c6-7ba273cb0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas df to get X and y\n",
    "pandas_df = final_df.toPandas()\n",
    "X = pandas_df['features'].tolist()  # Convert Series of arrays to list of arrays\n",
    "y = pandas_df['label'].tolist()\n",
    "words = [row.word for row in df.select(\"word\").collect()]\n",
    "freq = np.array([row.log_frequency for row in df.select(\"log_frequency\").collect()])\n",
    "\n",
    "X = np.array(pandas_df['features'].tolist())  # Shape: (n_samples, 769)\n",
    "y = np.array(pandas_df['label'].tolist())     # Shape: (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d286c425-30d0-4a3b-9f05-256582be9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract to utils file\n",
    "def decompose_for_training(x_train, x_test, n_components=50):\n",
    "    # Separate frequency and embeddings\n",
    "    x_train_freq = x_train[:, :1]\n",
    "    x_train_emb = x_train[:, 1:]\n",
    "\n",
    "    x_test_freq = x_test[:, :1]\n",
    "    x_test_emb = x_test[:, 1:]\n",
    "    \n",
    "    # Apply Truncated SVD to embeddings\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=0)\n",
    "    x_train_emb_reduced = svd.fit_transform(x_train_emb)\n",
    "    x_test_emb_reduced = svd.transform(x_test_emb)\n",
    "\n",
    "    # Save the svd decomposer to .joblib file\n",
    "    with open(SVD_PATH, \"wb\") as f_svd:\n",
    "        joblib.dump(svd, f_svd, protocol=None)\n",
    "\n",
    "    # Recombine with frequency\n",
    "    x_train_reduced = np.hstack([x_train_freq, x_train_emb_reduced])\n",
    "    x_test_reduced = np.hstack([x_test_freq, x_test_emb_reduced])\n",
    "\n",
    "    return x_train_reduced, x_test_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118abe41-d511-4efa-968c-84c609141132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimized bin boundaries\n",
    "# Choose 20 candidate thresholds and split into low and high ranges\n",
    "quantiles = np.linspace(0.05, 0.95, 20)\n",
    "candidates = np.quantile(freq, quantiles)\n",
    "\n",
    "# Split into low and high ranges, avoiding complete overlap\n",
    "low_range = candidates[:10]   # 5th to 50th percentile\n",
    "high_range = candidates[10:]  # 50th to 95th percentile\n",
    "\n",
    "thresholds, _ = evaluate_thresholds(freq, y, low_range, high_range)\n",
    "low_threshold, high_threshold = thresholds\n",
    "\n",
    "if low_threshold is None or high_threshold is None:\n",
    "    # substitue hard coded values\n",
    "    low_threshold = 1.68\n",
    "    high_threshold = 4.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e72ca-bf22-4ce8-96d0-6c4610c048b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing set\n",
    "SVD_PATH = './svd_v1.joblib' # TODO: Move this to constants with a path\n",
    "x_train, x_test, y_train, y_test, words_train, words_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    words, \n",
    "    test_size=0.33, \n",
    "    random_state=0, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "x_train_reduced, x_test_reduced = decompose_for_training(x_train, x_test, N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04616fc-5e62-4d2b-93fc-235bf13d0f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and train\n",
    "low_freq_clf = GaussianNB()\n",
    "mid_freq_clf = LogisticRegression(class_weight=\"balanced\", max_iter=1000, C=C)\n",
    "high_freq_clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    class_weight='balanced',\n",
    "    random_state=0\n",
    ")\n",
    "          \n",
    "clf = HybridFrequencyBinaryClassifier(\n",
    "    low_mid_threshold=low_threshold,\n",
    "    mid_high_threshold=high_threshold,\n",
    "    low_freq_model=low_freq_clf,\n",
    "    mid_freq_model=mid_freq_clf,\n",
    "    high_freq_model=high_freq_clf\n",
    ")\n",
    "\n",
    "clf.fit(x_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dfdac8-911a-451e-8152-93cc78540347",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test_reduced)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "y_proba = clf.predict_proba(x_test_reduced)\n",
    "auc = roc_auc_score(y_true=y_test, y_score=y_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677048c-d2bc-4674-9c12-02b61bb0d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"confusion matrix:\")\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print(f\"accuracy: {acc}, f1: {f1}, AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c4d4d-87d8-40ab-b435-05af1d9a02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"word\": words_test,\n",
    "    \"frequency\": x_test_reduced[:, 0],\n",
    "    \"true\": y_test,\n",
    "    \"pred\": y_pred\n",
    "})\n",
    "\n",
    "false_negatives = results[(results[\"true\"] == 1) & (results[\"pred\"] == 0)]\n",
    "true_negatives = results[(results[\"true\"] == 0) & (results[\"pred\"] == 0)]\n",
    "false_positives = results[(results[\"true\"] == 0) & (results[\"pred\"] == 1)]\n",
    "true_positives = results[(results[\"true\"] == 1) & (results[\"pred\"] == 1)]\n",
    "print(f\"false negatives\\n{false_negatives.head()}\")\n",
    "print(f\"true negatives\\n{true_negatives.head()}\")\n",
    "print(f\"false positives\\n{false_positives.head()}\")\n",
    "print(f\"true positives\\n{true_positives.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a9664-511a-49e9-9f24-27ba2c6d9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create paths and filename constants for model and svd\n",
    "# TODO: Save model\n",
    "# Save model\n",
    "# with open(H_MODEL_PATH, \"wb\") as f_model:\n",
    "#     joblib.dump(clf, f_model, protocol=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
