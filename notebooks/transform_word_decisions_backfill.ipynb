{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b16803-03af-4363-b95b-9b29f2c8d48a",
   "metadata": {},
   "source": [
    "# Backfill: transform_word_decisions\n",
    "\n",
    "Part of historical backfill pipeline\n",
    "\n",
    "- One backfill pipeline run per year\n",
    "- Work in batches of one month\n",
    "- For each month:\n",
    "    - Get the filepaths of puzzles for that month\n",
    "    - Transform each puzzle into `word_decisions` table rows\n",
    "    - write to the bronze table\n",
    "    - perform validation checks and audit logs before and after each write op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162832ff-4821-46a3-9ed8-f249f4b97a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./00_setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7751ea0-964f-472b-a4ab-828f00c4862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as F\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a8663-694a-415f-9d01-1d91d41303ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fileutils import get_latest_wordlist, word_file_to_set, get_puzzle_paths\n",
    "from src.wordutils import get_letter_set_map, transform_puzzle_to_word_decisions_by_path\n",
    "from src.bronzeutils import bronze_schema, rows_to_bronze_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b7417-a61b-40d0-be91-f67033b12b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_filename, wordlist_version = get_latest_wordlist()\n",
    "wordlist = word_file_to_set(wordlist_filename)\n",
    "letter_set_map = get_letter_set_map(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4626c33-5f53-44c4-865b-acbc0c24af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_month(year: int, month: int) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns word_decision rows for all puzzles in the given year/month\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    puzzle_paths = get_puzzle_paths(year, month)\n",
    "    for puzzle_path in sorted(puzzle_paths):\n",
    "        curr_rows = transform_puzzle_to_word_decisions_by_path(puzzle_path,\n",
    "                                                               wordlist,\n",
    "                                                               letter_set_map,\n",
    "                                                               wordlist_version)\n",
    "        rows.extend(curr_rows)\n",
    "\n",
    "    return rows_to_bronze_df(rows, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19978db4-864a-4b44-bd1a-759427f29d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Parameterized _YEAR\n",
    "_YEAR = 2024\n",
    "for month in range(1, 13):\n",
    "    print(f\"Processing year {_YEAR}, month {month}...\")\n",
    "    df = process_month(_YEAR, month)\n",
    "    print(f\"{df.count()} rows\")\n",
    "\n",
    "    # TODO: create db if doesn't exist, write to table, audit & log, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d494a-fe9e-4789-8250-c6bf4d99a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TODOS / notes below this line ======="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf05df-c924-4d5b-a3f6-e633c07dc45a",
   "metadata": {},
   "source": [
    "- Create database if doesn't exist (parameterized db names?)\n",
    "- Write to table (using replaceWhere, MERGE, something else??)\n",
    "- one pipeline to backfill, another for daily ingestion\n",
    "- backfill runs for a year, one month at a time, with verification and audit steps\n",
    "- backfill gets the paths for a given month (`glob` locally, `dbutils.fs.ls()` in cloud), then reads in each puzzle one at a time, writing to in-memory rows, then writes to a dataframe, then uses `uses replaceWhere` with Delta\n",
    "- daily can use the delete + write pattern (or will `replaceWhere` work for this as well??)\n",
    "- helper methods: `get_puzzle_by_date`, `ingest_puzzle_by_date` (for daily), `get_puzzle_paths`, `get_puzzle_by_path`, `ingest_puzzle_by_path` (for backfill) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e8225-e2f7-42f8-a75f-7e544e1af946",
   "metadata": {},
   "source": [
    "- Backfill script validates as it goes, uses replaceWhere with delta runs for a given year only, one chunk at a time\n",
    "- Daily ingest script that writes one file for a specific day/month/year\n",
    "- Repurpose helper methods to write to table, create db if it doesn't exist ... again with local and dbx code paths??\n",
    "- Try to do all writes at once or find a batch size\n",
    "- Need a way to redo the run, 1 write per puzzle date? Is that efficient??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
